# -*- coding: utf-8 -*-
"""GEM2023 - Model Generations.ipynb

Automatically generated by Colaboratory.
"""

import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import pandas as pd
import numpy as np
import re, statistics, time
import textstat
import openai

#torch.cuda.empty_cache()
#torch_device = "cuda" if torch.cuda.is_available() else "cpu"

#import os
#os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"

openai.organization = ""
openai.api_key = ""

# Instruction List
instruct_story_dict = {
    "level1": "Write a story using the following prompt: ",
    "level2_fkg": "Write a story that is readable by Grade 2 learners using the following prompt: ",
    "level2_cefr": "Write a story that is readable by A2 learners using the following prompt:  ",
    "level3_fkg": "Write a story that is readable by Grade 2 learners in the Flesch-Kincaid Grade Level scale using the following prompt: ",
    "level3_cefr": "Write a story that is readable by A2 learners in the CEFR scale using the following prompt: ",
    "level4_fkg": "Write a story that is readable by Grade 2 learners in the Flesch-Kincaid Grade Level scale using the following prompt. The Flesch-Kincaid Grade scale considers the total words, total sentences, and total syllables in a text:  ",
    "level4_cefr": "Write a story that is readable by A2 learners in the CEFR scale using the following prompt. Text assessed as A2 level uses basic sentence patterns with memorised phrases, uses explicit information and limited number of information points:  "
}

instruct_simp_dict = {
    "level1": "Simplify the following narrative: ",
    "level2_fkg": "Simplify the following narrative for Grade 2 learners: ",
    "level2_cefr": "Simplify the following narrative for A2 learners:  ",
    "level3_fkg": "Simplify the following narrative for Grade 2 learners in the Flesch Kincaid Grade scale: ",
    "level3_cefr": "Simplify the following narrative for A2 learners in the CEFR scale: ",
    "level4_fkg": "Simplify the following narrative for Grade 2 readers in the Flesch-Kincaid Grade scale. The Flesch-Kincaid Grade scale looks at total words, total sentences, and total syllables in a text:  ",
    "level4_cefr": "Simplify the following narrative for A2 learners in the CEFR Scale. Text assessed as A2 level uses basic sentence patterns with memorised phrases, uses explicit information and limited number of information points:  "
}

def generate_gpt35(prompt_list, instruction):
  count = 0
  gpt_continuations = []

  for item in prompt_list:
    full_instruction = instruction + "\n\n" + item #append instruction and story prompt

    #Handle request timed out
    while True:
      try:
        response = openai.ChatCompletion.create(
          model = "gpt-3.5-turbo",
          messages=[{"role": "user", "content": full_instruction}],
          max_tokens = 300,
        )

        gpt_output = response["choices"][0]["message"]["content"]
        gpt_continuations.append(gpt_output)
        count+=1
        print(count)
        break

      except Exception as e:
        print("Sleeping due to timeout.")
        time.sleep(900)

    if count % 15 == 0:
      print("Sleeping for 15 minutes...")
      time.sleep(900)
      print("Resuming execution.")
  return gpt_continuations

def generate_gpt4(prompt_list, instruction):
  gpt_continuations = []

  for item in prompt_list:
    full_instruction = instruction + "\n\n" + item #append instruction and story prompt

    response = openai.ChatCompletion.create(
      model = "gpt-4",
      messages=[{"role": "user", "content": full_instruction}],
      max_tokens = 1000,
    )

    gpt_output = response["choices"][0]["message"]["content"]
    gpt_continuations.append(gpt_output)
  return gpt_continuations

def generate_flant5(prompt_list, instruction):
  flan_continuations = []

  from transformers import T5Tokenizer, T5ForConditionalGeneration

  tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-base")
  model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-base", device_map="auto")

  for item in prompt_list:
    full_instruction = instruction + "\n\n" + item #append instruction and story prompt
    input_ids = tokenizer(full_instruction, return_tensors="pt").input_ids.to("cuda")

    output_undecoded = model.generate(input_ids, do_sample=True, min_new_tokens=30,max_new_tokens=300, top_p=0.95)
    output_decoded = tokenizer.decode(output_undecoded[0],skip_special_tokens=True)
    output_decoded = output_decoded.replace(full_instruction,'')
    flan_continuations.append(output_decoded)

  return flan_continuations

def generate_bloomz(prompt_list, instruction):
  bloomz_continuations = []

  from transformers import AutoTokenizer, BloomForCausalLM

  tokenizer = AutoTokenizer.from_pretrained("bigscience/bloomz-3b")
  model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-3b").to("cuda")

  for item in prompt_list:
    full_instruction = instruction + "\n\n" + item #append instruction and story prompt

    input_ids = tokenizer(full_instruction, return_tensors="pt").input_ids.to("cuda")
    output_undecoded = model.generate(input_ids, do_sample=True, min_new_tokens=30, max_new_tokens=300, top_p=0.95)
    output_decoded = tokenizer.decode(output_undecoded[0],skip_special_tokens=True)
    output_decoded = output_decoded.replace(full_instruction,'')
    bloomz_continuations.append(output_decoded)

  return bloomz_continuations

def generate_llama2(prompt_list, instruction):
  llama2_continuations = []

  access_token = "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"
  tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf", token=access_token)
  model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", token=access_token)

  pipeline = transformers.pipeline("text-generation", model=model, tokenizer=tokenizer, torch_dtype=torch.float16, device_map="auto")
  counter = 0

  for item in prompt_list:
    full_instruction = instruction + "\n\n" + item #append instruction and story prompt

    sequence = pipeline(full_instruction,
                         do_sample=True,
                         top_p=0.95,
			min_new_tokens=30,
                         num_return_sequences=1,
                         eos_token_id=tokenizer.eos_token_id,
                         max_new_tokens=300)
    output_decoded = sequence[0]['generated_text'].replace(full_instruction,'').strip()
    output_decoded = output_decoded.replace(full_instruction,'')
    counter+= 1
    print(counter)

    llama2_continuations.append(output_decoded)

  return llama2_continuations

def generate_longform(prompt_list, instruction):
  longform_continuations = []

  from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

  model = AutoModelForSeq2SeqLM.from_pretrained("akoksal/LongForm-T5-XL").to("cuda")
  tokenizer = AutoTokenizer.from_pretrained("akoksal/LongForm-T5-XL")

  for item in prompt_list:
    full_instruction = instruction + "\n\n" + item #append instruction and story prompt

    input_ids = tokenizer(full_instruction, return_tensors="pt").input_ids.to("cuda")
    output_undecoded = model.generate(input_ids, do_sample=True, min_new_tokens=30,max_new_tokens=300, top_p=0.95)
    output_decoded = tokenizer.decode(output_undecoded[0],skip_special_tokens=True)
    output_decoded = output_decoded.replace(full_instruction,'')
    longform_continuations.append(output_decoded)

  return longform_continuations

def generate_dolly(prompt_list, instruction):
  dolly_continuations = []

  from transformers import AutoTokenizer, AutoModelForCausalLM

  tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-3b")
  model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-3b").to("cuda")

  for item in prompt_list:
    full_instruction = instruction + "\n\n" + item #append instruction and story prompt

    input_ids = tokenizer(full_instruction, return_tensors="pt").input_ids.to("cuda")
    output_undecoded = model.generate(input_ids, do_sample=True, min_new_tokens=30, max_new_tokens=300, top_p=0.95, pad_token_id=tokenizer.eos_token_id)
    output_decoded = tokenizer.decode(output_undecoded[0],skip_special_tokens=True)
    output_decoded = output_decoded.replace(full_instruction,'')
    dolly_continuations.append(output_decoded)

  return dolly_continuations


edia_df = pd.read_csv("edia_story_prompt_equalized.csv")

# Specify column name for GPT3.5 fragile requests
level = 'A2'
edia_df = edia_df[edia_df['label'] == level]
prompt_list = edia_df['prompt'].tolist()

# Calling generations
generations = {}
sample_prompt_list = ["Once upon a time in a long faraway tower."]

for title,instruction in instruct_story_dict.items():
  print(title)
  generations[title] = generate_gpt35(prompt_list, instruction)

generations_df = pd.DataFrame.from_dict(generations)

filename = level+'.csv'
generations_df.to_csv(filename,index=False)

